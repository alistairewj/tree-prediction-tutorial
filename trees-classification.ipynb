{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab install\n",
    "\n",
    "The below cell installs necessary packages if running this notebook from Google Colaboratory. You can read more about colab [here](https://colab.research.google.com/notebooks/welcome.ipynb). Colaboratory provides a free runtime to execute this notebook in. If you'd like to try it out, click the following link:\n",
    "\n",
    "[View in Colaboratory](https://colab.research.google.com/github/alistairewj/tree-prediction-tutorial/blob/master/trees-classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages if needed\n",
    "try:\n",
    "    import pydotplus\n",
    "except:\n",
    "    !pip install pydotplus\n",
    "\n",
    "import pydotplus\n",
    "\n",
    "# install graphviz\n",
    "try:\n",
    "    !apt-get install graphviz -y\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree based methods for prediction\n",
    "\n",
    "This tutorial is meant to be an introduction to tree based methods for prediction. We start with the most basic model, a decision tree, and work our way up to the more recent work on gradient boosting. We will be alternating between sample datasets available with scikit-learn and a medical dataset made available in this repository (and originally sourced from PhysioNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import errno\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import datasets\n",
    "import pydotplus\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# used to display trees\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a helper function (`plot_model_pred_2d`) which will help us plot (i) the decision that the tree makes as the background colour and (ii) the actual data coloured with their true outcome. So, if blue overlaps with blue on our plot, we can be fairly happy that the algorithm is working well. Note that while there are more rigorous and quantitative forms of evaluating models, one can gain a lot of insight from visualizing the data.\n",
    "\n",
    "In order to make the colormaps have the same color as the decision tree, we use a `make_colormap` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_colormap(seq):\n",
    "    \"\"\"Return a LinearSegmentedColormap\n",
    "    seq: a sequence of floats and RGB-tuples. The floats should be increasing\n",
    "    and in the interval (0,1).\n",
    "    \"\"\"\n",
    "    seq = [(None,) * 3, 0.0] + list(seq) + [1.0, (None,) * 3]\n",
    "    cdict = {'red': [], 'green': [], 'blue': []}\n",
    "    for i, item in enumerate(seq):\n",
    "        if isinstance(item, float):\n",
    "            r1, g1, b1 = seq[i - 1]\n",
    "            r2, g2, b2 = seq[i + 1]\n",
    "            cdict['red'].append([item, r1, r2])\n",
    "            cdict['green'].append([item, g1, g2])\n",
    "            cdict['blue'].append([item, b1, b2])\n",
    "    return matplotlib.colors.LinearSegmentedColormap('CustomMap', cdict)\n",
    "\n",
    "def plot_model_pred_2d(mdl, X, y, cm=None, cbar=True, xlabel=None, ylabel=None):\n",
    "    # look at the regions in a 2d plot\n",
    "    # based on scikit-learn tutorial plot_iris.html\n",
    "    \n",
    "    # get minimum and maximum values\n",
    "    x0_min = X[:, 0].min()\n",
    "    x0_max = X[:, 0].max()\n",
    "    x1_min = X[:, 1].min()\n",
    "    x1_max = X[:, 1].max()\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(x0_min, x0_max, 100),\n",
    "                         np.linspace(x1_min, x1_max, 100))\n",
    "\n",
    "    Z = mdl.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    if not cm:\n",
    "        # custom colormap\n",
    "        #e58139f9 - orange\n",
    "        #399de5e0 - to blue\n",
    "        s = list()\n",
    "\n",
    "        lo = np.array(matplotlib.colors.to_rgb('#e5813900'))\n",
    "        hi = np.array(matplotlib.colors.to_rgb('#399de5e0'))\n",
    "\n",
    "        for i in range(255):\n",
    "            s.append( list((hi-lo)*(float(i)/255)+lo) )\n",
    "        cm = make_colormap(s)\n",
    "    \n",
    "    # plot the contour - colouring different regions\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=cm)\n",
    "\n",
    "    # plot the individual data points - colouring by the *true* outcome\n",
    "    color = y.ravel()\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=color, edgecolor='k', linewidth=2,\n",
    "                marker='o', s=60, cmap=cm)\n",
    "    \n",
    "    if xlabel is not None:\n",
    "        plt.xlabel(xlabel)\n",
    "    if ylabel is not None:\n",
    "        plt.ylabel(ylabel)\n",
    "    plt.axis(\"tight\")\n",
    "    #plt.clim([-1.5,1.5])\n",
    "    if cbar:\n",
    "        plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(mdl, cmap=None):\n",
    "    # cmap is a colormap\n",
    "    # e.g. cmap = matplotlib.cm.coolwarm( np.linspace(0.0, 1.0, 256, dtype=float) )\n",
    "    tree_graph = tree.export_graphviz(mdl, out_file=None,\n",
    "                             feature_names=feat, \n",
    "                             filled=True, rounded=True)\n",
    "    graph = pydotplus.graphviz.graph_from_dot_data(tree_graph)\n",
    "    \n",
    "    # get colormap\n",
    "    if cmap:\n",
    "        # remove transparency\n",
    "        if cmap.shape[1]==4:\n",
    "            cmap = cmap[:,0:2]\n",
    "        \n",
    "        nodes = graph.get_node_list()\n",
    "        for node in nodes:\n",
    "            if node.get_label():\n",
    "                # get number of samples in group 1 and group 2\n",
    "                num_samples = [int(ii) for ii in node.get_label().split('value = [')[1].split(']')[0].split(',')]\n",
    "\n",
    "                # proportion that is class 2\n",
    "                cm_value = float(num_samples[1]) / float(sum(num_samples))\n",
    "                # convert to (R, G, B, alpha) tuple\n",
    "                cm_value = matplotlib.cm.coolwarm(cm_value)\n",
    "                cm_value = [int(np.ceil(255*x)) for x in cm_value]\n",
    "                color = '#{:02x}{:02x}{:02x}'.format(cm_value[0], cm_value[1], cm_value[2])\n",
    "                node.set_fillcolor(color)\n",
    "\n",
    "    Image(graph.create_png())\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset we'll use is a classic: Fisher's iris. This data was collected by Edgar Anderson and was used by Fisher to demonstrate Linear Discriminant Analysis (LDA). We won't talk about LDA in this tutorial but it's an interesting technique worth learning about!\n",
    "\n",
    "The iris dataset includes the petal and sepal measurements for three types of flowers. The below code:\n",
    "\n",
    "* loads in the dataset\n",
    "* prints out a brief description\n",
    "* extracts two columns of the data into `X` \n",
    "* extracts the class labels into `y`\n",
    "\n",
    "Note that we only use two columns of data because we'd like to visualize the classifier. Also, we only use data from index `50` onward because we'd like to focus on two plants: versicolour and virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real example\n",
    "df = datasets.load_iris()\n",
    "\n",
    "# if you want a description of the dataset, uncomment the below line\n",
    "print(df['DESCR'])\n",
    "\n",
    "idx = [0,2]\n",
    "X = df['data'][50:,idx]\n",
    "y = df['target'][50:]\n",
    "\n",
    "# recode target as -1 and 1\n",
    "y[y==1] = -1\n",
    "y[y==2] =  1\n",
    "\n",
    "feat = [df['feature_names'][x] for x in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "Let's build the simplest tree model we can think of: a classification tree with only one split. Decision trees of this form are commonly referred to under the umbrella term Classification and Regression Trees (CART) [1]. While we will only be looking at classification here, regression isn't too different. After grouping the data (which is essentially what a decision tree does), classification involves assigning all members of the group to the majority class of that group during training. Regression is the same, except you would assign the average value, not the majority. In the case of a decision tree with one split, often called a \"stump\", the model will partition the data into two groups, and assign classes for those two groups based on majority vote. There are many parameters available for the `DecisionTreeClassifier` class; by specifying `max_depth=1` we will build a decision tree with only one split - i.e. of depth 1.\n",
    "\n",
    "[1] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify max_depth=1 so we train a stump, i.e. a tree with only 1 split\n",
    "mdl = tree.DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# fit the model to the data - trying to predict y from X\n",
    "mdl = mdl.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model is so simple, we can actually look at the full decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = create_graph(mdl)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see three nodes: a white node at the top, an orange node in the lower left, and a blue node in the lower right.\n",
    "\n",
    "The top (white) node is the root of the tree: it contains all the data. Let's read this node bottom to top, starting with `value = [50, 50]`. This line is telling us the current class balance: there are `50` observations of class 1, and `50` observations of class 2. In our iris data, that translates to `50` versicolour and `50` virginica. Moving up a line, `samples = 100` reminds us how many rows of data are assessed at this node. Since it's the root of our tree, all the data is assessed. Moving up again we have `gini = 0.5`. This is the Gini index, and is very important as it is the value that is used to split data. The Gini index is a measure of impurity - the higher the value is, the bigger mix of classes that you have. Right now we have a 50/50 split of two classes, and as a result, the gini index is `0.5`: about as bad as we can do. Ideally, we'd want this value to be 0, which would indicate that only one class of data is present in that node.\n",
    "\n",
    "Finally, the top line in the white node is the decision rule that has been learned for that node. That is, this line indicates what value the node will split the data on. In this case we've moved observations with petal length <= 4.75 cm to the left node, and implicitly all observations with petal length > 4.75 cm are moved into the right node. Looking in the two nodes, we can also see that the value for `gini` is lower in both, indicating that these nodes are more homogeneous. Looking at the `value` line, we can see that the left node has 44 observations in class 1, and 1 observation in class 2. This is much better than the 50/50 split we had earlier!\n",
    "\n",
    "It's nice to look at the `gini` coefficient because that's actually what the algorithm uses to determine a split. It evaluates every single feature (petal length, etc) at every possible split (4.75, 4.76, ...) to find the split which minimizes the gini in the two resultant nodes. Evaluating every possibility at each iteration and picking the best one is called a *greedy* approach.\n",
    "\n",
    "Let's take a look at what this decision boundary actually looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the regions in a 2d plot\n",
    "# based on scikit-learn tutorial plot_iris.html\n",
    "plt.figure(figsize=[10,8])\n",
    "plot_model_pred_2d(mdl, X, y, xlabel=feat[0], ylabel=feat[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the decision boundary of 4.75 cm for petal length (y-axis). We can also see a blue circle on the far left - the 1 point we misclassified as class 2 which had petal length < 4.75cm.\n",
    "\n",
    "Of course we are using a very simple model - let's see what happens when we increase the depth to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = tree.DecisionTreeClassifier(max_depth=5)\n",
    "mdl = mdl.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,8])\n",
    "plot_model_pred_2d(mdl, X, y, xlabel=feat[0], ylabel=feat[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our tree is more complicated - we can see a few vertical boundaries as well as the horizontal one from before. Some of these we may like - for example the movement of the boundary upward around septal length of ~6.7 cm. However, some appear unnatural; the vertical bar of classification done around a septal length of 6.1 cm, for example. Let's look at the tree itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_graph = tree.export_graphviz(mdl, out_file=None,\n",
    "                         feature_names=feat, \n",
    "                         filled=True, rounded=True)  \n",
    "graph = pydotplus.graphviz.graph_from_dot_data(tree_graph) \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the far right we see our culprit.\n",
    "\n",
    "* value = [1, 41] -> Split on Sepal length <= 6.05, go left\n",
    "* value = [1, 4] -> Split on Sepal Length <= 5.95, go *right* (i.e. sepal length > 5.95)\n",
    "* value = [1, 0] -> gini to 0.0, hurray!\n",
    "\n",
    "i.e. if the sepal length <= 6.05, and it is > 5.95, we classify it as class 1 (orange), otherwise it's class 0. Having an entire rule based upon this one observation seems silly, but it's perfectly logical as at the moment the **only** objective the algorithm cares about is minimizing the gini - and a `gini = 0.0` is lower than a `gini = 0.0465` - so it continues splitting until it gets there.\n",
    "\n",
    "This is where \"pruning\" the tree comes in (trust me, the gardening influenced play on words is just starting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(dt, min_samples_leaf = 1):\n",
    "    # Pruning is done by the \"min_samples_leaf\" property of decision trees\n",
    "    if dt.min_samples_leaf >= min_samples_leaf:\n",
    "        print('Decision tree is pruned at an equal or higher level.')\n",
    "    else:\n",
    "        # update prune parameter\n",
    "        dt.min_samples_leaf = min_samples_leaf\n",
    "        \n",
    "        # loop through each node of the tree\n",
    "        tree = dt.tree_\n",
    "        for i in range(tree.node_count):\n",
    "            n_samples = tree.n_node_samples[i]\n",
    "            if n_samples <= min_samples_leaf:\n",
    "                # we can't delete splits because they are fixed in the model\n",
    "                # instead, we remove the split by setting the child values to -1\n",
    "                tree.children_left[i]=-1\n",
    "                tree.children_right[i]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's prune the model and look again\n",
    "prune(mdl, min_samples_leaf = 10)\n",
    "tree_graph = tree.export_graphviz(mdl, out_file=None,\n",
    "                         feature_names=feat, \n",
    "                         filled=True, rounded=True)  \n",
    "graph = pydotplus.graphviz.graph_from_dot_data(tree_graph) \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that our second tree is (1) smaller in depth, and (2) never splits a node with <= 10 samples. We can look at the decision surface for this tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,8])\n",
    "plot_model_pred_2d(mdl, X, y, xlabel=feat[0], ylabel=feat[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pruned decision tree has a much more intuitive boundary, but does make some errors. We have reduced our performance in an effort to simplify the tree. This is the classic machine learning problem of trading off complexity with error.\n",
    "\n",
    "Note that, in order to do this, we \"invented\" the minimum samples per leaf node of 10. Why 10? Why not 5? Why not 20? The answer is: it depends on the dataset. Heuristically choosing these parameters can be time consuming, and we will see later on how gradient boosting elegantly handles this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees have high \"variance\"\n",
    "\n",
    "Before we move on to boosting, it will be useful to demonstrate how decision trees have high \"variance\". In this context, variance refers to a property of some models to have a wide range of performance given random samples of data. Let's take a look at randomly slicing the data we have too see what that means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "fig = plt.figure(figsize=[12,3])\n",
    "for i in range(3):\n",
    "    ax = fig.add_subplot(1,3,i+1)\n",
    "    \n",
    "    # generate indices in a random order\n",
    "    idx = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    # only use the first 50\n",
    "    idx = idx[:50]\n",
    "    X_temp = X[idx,:]\n",
    "    y_temp = y[idx]\n",
    "    \n",
    "    # initialize the model\n",
    "    mdl = tree.DecisionTreeClassifier(max_depth=5)\n",
    "    \n",
    "    # train the model using the dataset\n",
    "    mdl = mdl.fit(X_temp, y_temp)\n",
    "    \n",
    "    # only specify labels once for clarity\n",
    "    xlabel = feat[0] if i == 1 else None\n",
    "    ylabel = feat[1] if i == 0 else None\n",
    "        \n",
    "    plot_model_pred_2d(mdl, X_temp, y_temp, xlabel=xlabel, ylabel=ylabel, cbar=False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that we are using random subsets of data, and as a result, our decision boundary can change quite a bit. As you could guess, we actually don't want a model that randomly works well and randomly works poorly, so you may wonder why this is useful. The trick is that by combining many of instances of \"high variance\" classifiers (decision trees), we can end up with a single classifier with low variance. There is an old joke: two farmers and a statistician go hunting. They see a deer: the first farmer shoots, and misses to the left. The next farmer shoots, and misses to the right. The statistician yells \"We got it!!\". \n",
    "\n",
    "While it doesn't quite hold in real life, it turns out that this principle does hold for decision trees - combining them in the right way ends up building powerful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "The premise of boosting is the combination of many weak learners to form a single \"strong\" learner. In a nutshell, boosting involves building a models iteratively, and at each step we focus on the data we performed poorly on. In our context, we'll use decision trees, so the first step would be to build a tree using the data. Next, we'd look at the data that we misclassified, and re-weight the data so that we really wanted to classify those observations correctly, at a cost of maybe getting some of the other data wrong this time. Let's see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "clf = tree.DecisionTreeClassifier(max_depth=1)\n",
    "mdl = ensemble.AdaBoostClassifier(base_estimator=clf,n_estimators=6)\n",
    "mdl = mdl.fit(X,y)\n",
    "\n",
    "# plot each individual decision tree\n",
    "fig = plt.figure(figsize=[10,5])\n",
    "for i, estimator in enumerate(mdl.estimators_):\n",
    "    xlabel = feat[0] if i == 4 else None\n",
    "    ylabel = feat[1] if i % 3 == 0 else None\n",
    "    ax = fig.add_subplot(2,3,i+1)\n",
    "    plot_model_pred_2d(estimator, X, y, xlabel=xlabel, ylabel=ylabel, cbar=False)\n",
    "    txt = 'Tree {}'.format(i+1)\n",
    "    plt.text(7.0, 3.5, txt, fontdict={'fontsize':12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above, we can see that the first iteration builds the exact same simple decision tree as we had seen earlier. This makes sense - it's using the entire dataset with no special weighting. \n",
    "\n",
    "In the next iteration we can see the model shift - it misclassified five observations in class 1, and now these are the most important observations. Consequently, it picks the boundary that, while prioritizing correctly classifies these observations, still tries to best classify the rest of the data too. Now we have correctly classified all but one observation, the one on the far left middle of the graph. In iteration 3, the algorithm solely focuses on correctly classifying this one observation.\n",
    "\n",
    "One important point is that each tree is weighted by it's global error. In the figure above, it's obvious that we wouldn't want to weight Tree 3 equally to Tree 1, when Tree 1 is doing so much better overall. It turns out that weighting each tree by the inverse of its error is a pretty good way to do this.\n",
    "\n",
    "Let's look at final model's decision surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the final prediction\n",
    "plt.figure(figsize=[9,5])\n",
    "plot_model_pred_2d(mdl, X, y, xlabel=feat[0], ylabel=feat[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's AdaBoost! There are a few tricks we have glossed over here - but you understand the general principle. Now we'll move on to a different approach. With boosting, we iteratively changed the dataset to have new trees focus on the \"difficult\" observations. The next approach we discuss is similar as it also involves using changed versions of our dataset to build new trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bootstrap aggregation, or \"Bagging\", is another form of *ensemble learning* where we aim to build a single good model by combining many models together. With AdaBoost, we modified the data to focus on hard to classify observations. We can imagine this as a form of resampling the data for each new tree. For example, say we have three observations: A, B, and C, `[A, B, C]`. If we correctly classify observations `[A, B]`, but incorrectly classify `C`, then AdaBoost involves building a new tree that focuses on `C`. Equivalently, we could say AdaBoost builds a new tree using the dataset `[A, B, C, C, C]`, where we have *intentionally* repeated observation `C` 3 times so that the algorithm thinks it is 3 times as important as the other observations. Before we move on, convince yourself that this makes sense.\n",
    "\n",
    "Bagging involves the exact same approach, except we don't selectively choose which observations to focus on, but rather we *randomly select subsets of data each time*. As you can see, while this is a similar process to AdaBoost, the concept is quite different. Whereas before we aimed to iteratively improve our overall model with new trees, we now build trees on what we hope are independent datasets.\n",
    "\n",
    "Let's take a step back, and think about a practical example. Say we wanted a good model of heart disease. If we saw researchers build a model from a dataset of patients from their hospital, we would be happy. If they then acquired a new dataset from new patients, and built a new model, we'd be inclined to feel that the combination of the two models would be better than any one individually. This exact scenario is what bagging aims to replicate, except instead of actually going out and collecting new datasets, we instead use bootstraping to create new sets of data from our current dataset. If you are unfamiliar with bootstrapping, you can treat it as \"magic\" for now (and if you are familiar with the bootstrap, you already know it's magic).\n",
    "\n",
    "Let's take a look at a simple bootstrap model with the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(321)\n",
    "clf = tree.DecisionTreeClassifier(max_depth=5)\n",
    "mdl = ensemble.BaggingClassifier(base_estimator=clf, n_estimators=6)\n",
    "mdl = mdl.fit(X,y)\n",
    "\n",
    "fig = plt.figure(figsize=[10,5])\n",
    "for i, estimator in enumerate(mdl.estimators_):\n",
    "    xlabel = feat[0] if i == 4 else None\n",
    "    ylabel = feat[1] if i % 3 == 0 else None\n",
    "    \n",
    "    ax = fig.add_subplot(2,3,i+1)\n",
    "    plot_model_pred_2d(estimator, X, y, xlabel=xlabel, ylabel=ylabel, cbar=False)\n",
    "    txt = 'Tree {}'.format(i+1)\n",
    "    plt.text(7.0, 3.5, txt, fontdict={'fontsize':12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each individual tree is quite variable - this is a result of using a random set of data to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the final prediction\n",
    "plt.figure(figsize=[8,5])\n",
    "plot_model_pred_2d(mdl, X, y, xlabel=feat[0], ylabel=feat[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Of course, since this is a simple dataset, we are not seeing that many dramatic changes between different models. Don't worry, we'll quantitatively evaluate them later.\n",
    "\n",
    "Next up, a minor addition creates one of the most popular models in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we used bagging to randomly resample our data to generate \"new\" datasets to build trees from. The Random Forest takes this one step further: instead of just resampling our data, we also select only a fraction of the features to include. It turns out that this subselection tends to improve the performance of our models. The odds of an individual being very good or very bad is higher (i.e. the variance of the trees is increased), and this ends up giving us a final model with better overall performance (lower bias).\n",
    "\n",
    "Let's train the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(321)\n",
    "mdl = ensemble.RandomForestClassifier(max_depth=5, n_estimators=6, max_features=1)\n",
    "mdl = mdl.fit(X,y)\n",
    "\n",
    "fig = plt.figure(figsize=[12,8])\n",
    "for i, estimator in enumerate(mdl.estimators_):\n",
    "    \n",
    "    xlabel = feat[0] if i == 4 else None\n",
    "    ylabel = feat[1] if i % 3 == 0 else None\n",
    "    \n",
    "    ax = fig.add_subplot(2,3,i+1)\n",
    "    plot_model_pred_2d(estimator, X, y, xlabel=xlabel, ylabel=ylabel, cbar=False)\n",
    "    txt = 'Tree {}'.format(i+1)\n",
    "    plt.text(7.0, 3.5, txt, fontdict={'fontsize':12})\n",
    "plt.show()\n",
    "\n",
    "print('\\nPlotting final decision surface\\n')\n",
    "plt.figure(figsize=[9,5])\n",
    "plot_model_pred_2d(mdl, X, y, xlabel=feat[0], ylabel=feat[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the visualization doesn't *really* show us the power of Random Forests, but we'll quantitatively evaluate them soon enough.\n",
    "\n",
    "Last, and not least, we move on to gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Gradient boosting (GB) is our last topic - and elegantly combines concepts from the previous methods. \n",
    "As a \"boosting\" method, GB involves iteratively building trees, aiming to improve upon misclassifications of the previous tree. GB also borrows the concept of sub sampling the number of columns (as was done in Random Forests), which tends to prevent overfitting.\n",
    "\n",
    "While it is hard to express in this non-technical tutorial, the biggest innovation in GB is that it provides a unifying mathematical framework for boosting models.\n",
    "GB explicitly casts the problem of building a tree as an optimization problem, defining mathematical functions for how well a tree is performing (which we had before) *and* how complex a tree is. In this light, one can actually treat AdaBoost as a \"special case\" of GB, where the loss function is chosen to be the exponential loss.\n",
    "\n",
    "Let's build a GB model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(321)\n",
    "mdl = ensemble.GradientBoostingClassifier(n_estimators=10)\n",
    "mdl = mdl.fit(X, y)\n",
    "\n",
    "print('\\nPlotting final decision surface\\n')\n",
    "plt.figure(figsize=[9,5])\n",
    "plot_model_pred_2d(mdl, X, y, xlabel=feat[0], ylabel=feat[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running through a slightly harder dataset\n",
    "\n",
    "We've now learned the basics of the various tree methods and have visualized most of them on the Fisher iris data. We now move on to a harder classification problem involving the identification of breast cancer tumours from features describing cell nuclei of breast mass. The goal is to classify whether the mass is cancerous or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# real example\n",
    "df_bc = datasets.load_breast_cancer()\n",
    "\n",
    "# if you want a description of the dataset, uncomment the below line\n",
    "print(df_bc['DESCR'])\n",
    "\n",
    "# pick index of the features to use (only pick 2)\n",
    "#    :Attribute Information (in order):\n",
    "#        0 - CRIM     per capita crime rate by town\n",
    "#        1 - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "#        2 - INDUS    proportion of non-retail business acres per town\n",
    "#        3 - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "#        4 - NOX      nitric oxides concentration (parts per 10 million)\n",
    "#        5 - RM       average number of rooms per dwelling\n",
    "#        6 - AGE      proportion of owner-occupied units built prior to 1940\n",
    "#        7 - DIS      weighted distances to five Boston employment centres\n",
    "#        8 - RAD      index of accessibility to radial highways\n",
    "#        9 - TAX      full-value property-tax rate per $10,000\n",
    "#       10 - PTRATIO  pupil-teacher ratio by town\n",
    "#       11 - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "#       12 - LSTAT    % lower status of the population\n",
    "#       13 - MEDV     Median value of owner-occupied homes in $1000's\n",
    "\n",
    "\n",
    "idx = [1,29]\n",
    "X = df_bc['data'][:,idx]\n",
    "y = df_bc['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.30, random_state=42)\n",
    "\n",
    "feat = [x for x in df_bc['feature_names'][idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we've extracted two features out of the breast cancer dataset. We'll use these to quickly visualize all the models we have presented here. We'll also print out their performance as measured by the AUROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = dict()\n",
    "clf['Decision Tree'] = tree.DecisionTreeClassifier(criterion='entropy', splitter='best').fit(X_train,y_train)\n",
    "clf['Gradient Boosting'] = ensemble.GradientBoostingClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "clf['Random Forest'] = ensemble.RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "clf['Bagging'] =  ensemble.BaggingClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "clf['AdaBoost'] =  ensemble.AdaBoostClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "\n",
    "fig = plt.figure(figsize=[16,9])\n",
    "\n",
    "print('AUROC\\tModel')\n",
    "for i, curr_mdl in enumerate(clf):\n",
    "    xlabel = feat[0] if i == 4 else None\n",
    "    ylabel = feat[1] if i == 0 else None\n",
    "    \n",
    "    yhat = clf[curr_mdl].predict_proba(X_test)[:,1]\n",
    "    score = metrics.roc_auc_score(y_test, yhat)\n",
    "    print('{:0.3f}\\t{}'.format(score, curr_mdl))\n",
    "    ax = fig.add_subplot(2,3,i+1)\n",
    "    \n",
    "    plot_model_pred_2d(clf[curr_mdl], X_test, y_test, xlabel=xlabel, ylabel=ylabel, cbar=False)\n",
    "    \n",
    "    plt.title(curr_mdl)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that quantitatively, AdaBoost and Gradient Boosting have produced the highest discrimination among all the models (~0.80). The decision surfaces of these models also seem simpler, and less \"noisy\", which tends to result in improved generalization on the held out test set (this is not a rule, just Occam's razor in action).\n",
    "\n",
    "Now we will include all features of the breast cancer dataset. As a result, we will no longer be able to easily visualize the models, but we will be able to better evaluate them. We'll also switch to using 5-fold cross-validation to get a good estimate of the generalization performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_bc['data']\n",
    "y = df_bc['target']\n",
    "\n",
    "# use cross-validation to estimate the performance of each model\n",
    "print('Acc\\tAUROC\\tModel')\n",
    "for curr_mdl in clf:\n",
    "    scores = cross_val_score(clf[curr_mdl], X, y, cv=5, scoring='accuracy')\n",
    "    auc = cross_val_score(clf[curr_mdl], X, y, cv=5, scoring='roc_auc')\n",
    "    print('{:0.3f}\\t{:0.3f}\\t{}'.format(scores.mean(), auc.mean(), curr_mdl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note two things:\n",
    "\n",
    "* by using the entire feature set we have dramatically improved performance of the model, indicating that there was more information contained in the other columns\n",
    "* most of our models are performing relatively equivalently (except for the super simple model, the decision tree).\n",
    "\n",
    "To make appropriate comparisons, we should calculate 95% confidence intervals on these performance estimates. This can be done a number of ways; the easiest is to bootstrap the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "We'll now practice using these models on a dataset acquired from patients admitted to intensive care units at the Beth Israel Deaconness Medical Center in Boston, MA. All patients in the cohort stayed for at least 48 hours, and the goal of the prediction task is to predict in-hospital mortality. This data is a subset of a publicly accessible ICU database: MIMIC. If you're interested, you can read more about MIMIC [here](https://mimic.physionet.org).\n",
    "The particular dataset we are using is described in more detail here: http://physionet.org/challenge/2012/\n",
    "\n",
    "The data is originally provided as a time series of observations for a number of variables, but to simplify the analysis, we've done some preprocessing to get a single row for each patient. \n",
    "The following cell will check if the data is available here. If it's not, it will download it to the subfolder `data` in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('data','PhysionetChallenge2012_data.csv.zip')\n",
    "\n",
    "# if we don't have the data, we will download it here\n",
    "if not os.path.exists(data_path):\n",
    "    from six.moves import urllib\n",
    "    url = 'https://github.com/alistairewj/tree-prediction-tutorial/raw/master/data/PhysionetChallenge2012_data.csv.zip'\n",
    "    \n",
    "    # make a data subfolder\n",
    "    try:\n",
    "        os.makedirs('data')\n",
    "    except OSError as e:\n",
    "        # if the data subfolder already exists, pass the error\n",
    "        if e.errno == errno.EEXIST:\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # make a request and download the data file\n",
    "    data = urllib.request.urlopen(url)\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(data.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "df = pd.read_csv(data_path, compression='zip', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "X = df.values\n",
    "\n",
    "# here we only use the first 3000 observations as our training set\n",
    "y_train = X[0:3000, 0]\n",
    "X_train = X[0:3000, 1:]\n",
    "\n",
    "y_test = X[3000:, 0]\n",
    "X_test = X[3000:, 1:]\n",
    "\n",
    "print('Training size: {} - {:6d} missing observations'.format(X_train.shape,\n",
    "                                                           np.sum(np.sum(np.isnan(X_train)))))\n",
    "print('Test size:     {} - {:6d} missing observations'.format(X_test.shape,\n",
    "                                 np.sum(np.sum(np.isnan(X_test)))))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outcome is the first column `'hospitalmortality'`. The rest of the data are features you can use to predict this binary outcome.\n",
    "\n",
    "Now, using what you've learned above, try to build the five classifiers using decision trees, AdaBoost, Random Forest, Bagging, and Gradient Boosting. Pick your favourite and play with the parameters to see how well you can do! Be sure to use cross-validation to make sure you don't overfit.\n",
    "\n",
    "WARNING: You'll note that the above has a lot of missing data! We haven't dealt with this before, but it is a challenging issue with medical data. In general there are three types of missing data:\n",
    "\n",
    "1. Missing completely at random (MCAR)\n",
    "    * The data is missing for reasons *unrelated* to the data\n",
    "    * a power outage results in losing vital sign data\n",
    "2. Missing at random (MAR)\n",
    "    * The data is missing for reasons related to the data, but not the missing observation\n",
    "    * we don't collect lactate measurements on admission to a medical ICU, but we collect them for cardiac ICU\n",
    "3. Missing not at random (MNAR)\n",
    "    * The data is missing, and the reason it is missing *depends* on the value\n",
    "    * a doctor does not order the Troponin-I lab test, because they believe it to be normal\n",
    "  \n",
    "  \n",
    "The hardest case to deal with is MNAR, and unfortunately, that is the most common in the medical domain. Still, we have to do something, so we often use approaches which are theoretically invalid under MNAR but in practice work acceptably well.\n",
    "\n",
    "Below, we'll replace missing data with the average value for the training population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing data\n",
    "\n",
    "# since decision trees do not handle missing data, we impute it here\n",
    "mu = np.nanmean(X_train,axis=1)\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    idxMissing = np.isnan(X_train[:,i])\n",
    "    X_train[idxMissing, i] = mu[i]\n",
    "    \n",
    "    idxMissing = np.isnan(X_test[:,i])\n",
    "    X_test[idxMissing, i] = mu[i]\n",
    "\n",
    "# now we should find that we have no more missing data!\n",
    "\n",
    "print('Training size: {} - {:6d} missing observations'.format(X_train.shape,\n",
    "                                                           np.sum(np.sum(np.isnan(X_train)))))\n",
    "print('Test size:     {} - {:6d} missing observations'.format(X_test.shape,\n",
    "                                 np.sum(np.sum(np.isnan(X_test)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the missing data is handled, try to build the above tree models using the ICU data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pn = dict()\n",
    "auc = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a decision tree classifier\n",
    "mdl_dt = tree.DecisionTreeClassifier(criterion='entropy', splitter='best')\n",
    "\n",
    "# Fit the model to the training data\n",
    "mdl_dt = mdl_dt.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "yhat = mdl_dt.predict_proba(X_test)[:,1]\n",
    "score = metrics.roc_auc_score(y_test, yhat)\n",
    "\n",
    "# Add the trained model to our dictionary which has all the models\n",
    "clf_pn['Decision Tree'] = mdl_dt\n",
    "\n",
    "# Add the AUROC to an AUROC dictionary\n",
    "auc['Decision Tree'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an AdaBoost classifier\n",
    "mdl_adaboost = ensemble.AdaBoostClassifier(n_estimators=10)\n",
    "\n",
    "# Fit the model to the training data\n",
    "mdl_adaboost = mdl_adaboost.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "yhat = mdl_adaboost.predict_proba(X_test)[:,1]\n",
    "score = metrics.roc_auc_score(y_test, yhat)\n",
    "\n",
    "# Add the trained model to our dictionary which has all the models\n",
    "clf_pn['AdaBoost'] = mdl_dt\n",
    "\n",
    "# Add the AUROC to an AUROC dictionary\n",
    "auc['AdaBoost'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a random forest\n",
    "mdl_rf = ensemble.RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "# Fit the model to the training data\n",
    "mdl_rf = mdl_rf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "yhat = mdl_rf.predict_proba(X_test)[:,1]\n",
    "score = metrics.roc_auc_score(y_test, yhat)\n",
    "\n",
    "# Add the trained model to our dictionary which has all the models\n",
    "clf_pn['Random Forest'] = mdl_rf\n",
    "\n",
    "# Add the AUROC to an AUROC dictionary\n",
    "auc['Random Forest'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Gradient Boosting classifier\n",
    "mdl_gb = ensemble.GradientBoostingClassifier(n_estimators=10)\n",
    "\n",
    "# Fit the model to the training data\n",
    "mdl_gb = mdl_gb.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the test set\n",
    "yhat = mdl_gb.predict_proba(X_test)[:,1]\n",
    "score = metrics.roc_auc_score(y_test, yhat)\n",
    "\n",
    "# Add the trained model to our dictionary which has all the models\n",
    "clf_pn['Gradient Boosting'] = mdl_gb\n",
    "\n",
    "# Add the AUROC to an AUROC dictionary\n",
    "auc['Gradient Boosting'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the 4 above models\n",
    "print('AUROC\\tModel')\n",
    "for i, curr_mdl in enumerate(clf_pn):\n",
    "    print('{:0.3f}\\t{}'.format(auc[curr_mdl], curr_mdl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep exploring!\n",
    "\n",
    "The above models work fairly well, but there is room for improvement!\n",
    "\n",
    "* Are there other ways to impute missing data?\n",
    "* Have we thought about the features in our data, and how we are using them?\n",
    "* Have we visualized the data? Are there any obvious outliers which may fool our model?\n",
    "* Are there parameters of our model which we could change?\n",
    "* Is there a systematic way of choosing the parameters of our model?\n",
    "\n",
    "The below code downloads a second set of data - `set-b`. This is the same type of data from a distinct 4000 patients, but this time you don't have the answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-tutorial",
   "language": "python",
   "name": "tree-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
